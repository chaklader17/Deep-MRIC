{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Means Clustering for Brain Tumor Analysis\n",
        "\n",
        "This notebook applies K-Means clustering to brain MRI images for unsupervised analysis and pattern discovery.\n",
        "\n",
        "## Overview\n",
        "\n",
        "K-Means clustering is an **unsupervised learning** algorithm that groups similar images together without requiring labeled data. This notebook:\n",
        "1. Loads and preprocesses brain MRI images\n",
        "2. Extracts features from images (using flattening or feature extraction)\n",
        "3. Applies K-Means clustering to group similar images\n",
        "4. Visualizes clusters and cluster centers\n",
        "5. Evaluates clustering performance using metrics (inertia, silhouette score)\n",
        "6. Analyzes cluster assignments\n",
        "\n",
        "**Note:** K-Means is unsupervised - it doesn't use class labels during training, but we can compare cluster assignments with true labels for evaluation purposes.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Make sure you have installed all required packages:\n",
        "```bash\n",
        "pip install numpy pandas scikit-learn matplotlib seaborn opencv-python pillow tqdm\n",
        "```\n",
        "\n",
        "Or install from requirements.txt:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load images using CSV metadata to prevent data leakage\n",
        "print(\"Loading images using CSV metadata files to prevent data leakage...\")\n",
        "\n",
        "# Load metadata CSV files\n",
        "augmented_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "original_metadata_path = 'data/dataset_metadata.csv'\n",
        "\n",
        "if os.path.exists(augmented_metadata_path):\n",
        "    aug_metadata_df = pd.read_csv(augmented_metadata_path)\n",
        "    print(f\"Loaded augmented metadata: {len(aug_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {augmented_metadata_path} not found.\")\n",
        "    aug_metadata_df = pd.DataFrame()\n",
        "\n",
        "if os.path.exists(original_metadata_path):\n",
        "    orig_metadata_df = pd.read_csv(original_metadata_path)\n",
        "    print(f\"Loaded original metadata: {len(orig_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {original_metadata_path} not found.\")\n",
        "    orig_metadata_df = pd.DataFrame()\n",
        "\n",
        "# Filter training data to exclude test/val images\n",
        "if len(aug_metadata_df) > 0 and len(orig_metadata_df) > 0:\n",
        "    # Get test and val original filenames\n",
        "    test_originals = set(orig_metadata_df[orig_metadata_df['split'] == 'test']['filename'].unique())\n",
        "    val_originals = set(orig_metadata_df[orig_metadata_df['split'] == 'val']['filename'].unique())\n",
        "    excluded_originals = test_originals.union(val_originals)\n",
        "    \n",
        "    # Filter train data: only keep images whose original_filename is NOT in test/val\n",
        "    train_df = aug_metadata_df[aug_metadata_df['split'] == 'train'].copy()\n",
        "    train_df_filtered = train_df[~train_df['original_filename'].isin(excluded_originals)]\n",
        "    \n",
        "    print(f\"Filtered training data: {len(train_df_filtered)} rows (from {len(train_df)} total train rows)\")\n",
        "    print(f\"Excluded {len(train_df) - len(train_df_filtered)} rows that overlap with test/val sets\")\n",
        "    \n",
        "    # Load images from filtered metadata\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    \n",
        "    for _, row in tqdm(train_df_filtered.iterrows(), total=len(train_df_filtered), desc=\"Loading images\"):\n",
        "        # Use full_path if available, otherwise construct from DATA_DIR and image_path\n",
        "        if pd.notna(row.get('full_path')):\n",
        "            img_path = row['full_path']\n",
        "        else:\n",
        "            img_path = os.path.join(DATA_DIR, row['image_path'])\n",
        "        \n",
        "        # Normalize path separators\n",
        "        img_path = img_path.replace('\\\\', '/')\n",
        "        \n",
        "        if os.path.exists(img_path):\n",
        "            try:\n",
        "                # Read image in grayscale\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is not None:\n",
        "                    # Resize image\n",
        "                    img = cv2.resize(img, IMG_SIZE)\n",
        "                    images.append(img)\n",
        "                    labels.append(row['class'])\n",
        "                    image_paths.append(img_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    train_images = np.array(images)\n",
        "    train_labels = np.array(labels)\n",
        "    train_paths = np.array(image_paths)\n",
        "    \n",
        "    print(f\"Loaded {len(train_images)} filtered training images\")\n",
        "    USE_CSV_METADATA = True\n",
        "else:\n",
        "    print(\"Warning: CSV metadata not available. Will use directory loading (may have data leakage).\")\n",
        "    USE_CSV_METADATA = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples, adjusted_rand_score\n",
        "from sklearn.decomposition import PCA\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = 'data/vgg16_classification'\n",
        "MODEL_DIR = 'models/kmeans'\n",
        "MODEL_SAVE_PATH = os.path.join(MODEL_DIR, 'kmeans_clusterer.pkl')\n",
        "RESULTS_SAVE_PATH = os.path.join(MODEL_DIR, 'kmeans_results.csv')\n",
        "VISUALIZATION_DIR = os.path.join(MODEL_DIR, 'visualizations')\n",
        "\n",
        "# Clustering parameters\n",
        "N_CLUSTERS = 4  # Number of clusters (matching 4 classes: NO_TUMOR, GLIOMA, MENINGIOMA, PITUITARY)\n",
        "MAX_ITER = 300  # Maximum iterations for K-Means\n",
        "N_INIT = 10     # Number of times K-Means will run with different centroid seeds\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Image preprocessing parameters\n",
        "IMG_SIZE = (224, 224)  # Resize images to this size\n",
        "USE_FEATURE_EXTRACTION = True  # If True, use PCA for dimensionality reduction\n",
        "N_COMPONENTS = 50  # Number of PCA components (if USE_FEATURE_EXTRACTION is True)\n",
        "\n",
        "# Class names (for evaluation purposes)\n",
        "CLASS_NAMES = ['NO_TUMOR', 'GLIOMA', 'MENINGIOMA', 'PITUITARY']\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "print(f\"  Number of clusters: {N_CLUSTERS}\")\n",
        "print(f\"  Image size: {IMG_SIZE}\")\n",
        "print(f\"  Feature extraction: {USE_FEATURE_EXTRACTION}\")\n",
        "if USE_FEATURE_EXTRACTION:\n",
        "    print(f\"  PCA components: {N_COMPONENTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_images_from_directory(data_dir, max_samples=None):\n",
        "    \"\"\"\n",
        "    Load images from directory structure.\n",
        "    Returns: (images, labels, image_paths)\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    \n",
        "    # Get all class directories\n",
        "    class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "    \n",
        "    for class_name in class_dirs:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        image_files = [f for f in os.listdir(class_path) \n",
        "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        if max_samples:\n",
        "            image_files = image_files[:max_samples]\n",
        "        \n",
        "        for img_file in tqdm(image_files, desc=f\"Loading {class_name}\"):\n",
        "            img_path = os.path.join(class_path, img_file)\n",
        "            try:\n",
        "                # Read image in grayscale\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is not None:\n",
        "                    # Resize image\n",
        "                    img = cv2.resize(img, IMG_SIZE)\n",
        "                    images.append(img)\n",
        "                    labels.append(class_name)\n",
        "                    image_paths.append(img_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    return np.array(images), np.array(labels), np.array(image_paths)\n",
        "\n",
        "# Load images from training set\n",
        "# Use train_augmented directory (created by augment_training_data.py)\n",
        "# If train_augmented doesn't exist, fall back to train directory\n",
        "train_dir = os.path.join(DATA_DIR, 'train_augmented')\n",
        "if not os.path.exists(train_dir):\n",
        "    print(f\"Warning: {train_dir} not found. Using 'train' directory instead.\")\n",
        "    print(\"Run 'python augment_training_data.py' first to create augmented training data.\")\n",
        "    train_dir = os.path.join(DATA_DIR, 'train')\n",
        "else:\n",
        "    print(\"Using augmented training data from train_augmented directory\")\n",
        "\n",
        "print(\"Loading images from training set...\")\n",
        "train_images, train_labels, train_paths = load_images_from_directory(train_dir)\n",
        "\n",
        "print(f\"\\nLoaded {len(train_images)} images\")\n",
        "print(f\"Image shape: {train_images[0].shape}\")\n",
        "print(f\"Unique classes: {np.unique(train_labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Class Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display class distribution\n",
        "class_counts = pd.Series(train_labels).value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(class_counts.index, class_counts.values, \n",
        "               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "plt.title('Class Distribution in Training Set', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Class', fontsize=12)\n",
        "plt.ylabel('Number of Samples', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(class_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten images to feature vectors\n",
        "print(\"Extracting features from images...\")\n",
        "n_samples = train_images.shape[0]\n",
        "n_features = train_images.shape[1] * train_images.shape[2]\n",
        "\n",
        "# Flatten images\n",
        "X_flattened = train_images.reshape(n_samples, -1)\n",
        "print(f\"Flattened shape: {X_flattened.shape}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flattened)\n",
        "print(f\"Scaled shape: {X_scaled.shape}\")\n",
        "\n",
        "# Apply PCA for dimensionality reduction (optional)\n",
        "if USE_FEATURE_EXTRACTION:\n",
        "    print(f\"\\nApplying PCA with {N_COMPONENTS} components...\")\n",
        "    pca = PCA(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n",
        "    X_features = pca.fit_transform(X_scaled)\n",
        "    print(f\"PCA shape: {X_features.shape}\")\n",
        "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "    print(f\"  (First {N_COMPONENTS} components explain {pca.explained_variance_ratio_.sum()*100:.2f}% of variance)\")\n",
        "else:\n",
        "    X_features = X_scaled\n",
        "    pca = None\n",
        "\n",
        "print(f\"\\nFeature extraction complete\")\n",
        "print(f\"Final feature shape: {X_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. K-Means Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and fit K-Means\n",
        "print(f\"Training K-Means with {N_CLUSTERS} clusters...\")\n",
        "print(f\"Max iterations: {MAX_ITER}\")\n",
        "print(f\"Number of initializations: {N_INIT}\")\n",
        "\n",
        "kmeans = KMeans(\n",
        "    n_clusters=N_CLUSTERS,\n",
        "    max_iter=MAX_ITER,\n",
        "    n_init=N_INIT,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "kmeans.fit(X_features)\n",
        "\n",
        "print(\"\\nK-Means training completed!\")\n",
        "print(f\"Inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n",
        "print(f\"Number of iterations: {kmeans.n_iter_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Cluster Assignments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cluster assignments\n",
        "cluster_labels = kmeans.labels_\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "print(f\"Cluster assignments shape: {cluster_labels.shape}\")\n",
        "print(f\"Cluster centers shape: {cluster_centers.shape}\")\n",
        "\n",
        "# Count samples per cluster\n",
        "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "print(\"\\nSamples per cluster:\")\n",
        "for cluster_id, count in zip(unique, counts):\n",
        "    print(f\"  Cluster {cluster_id}: {count} samples\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'image_path': train_paths,\n",
        "    'true_label': train_labels,\n",
        "    'cluster_id': cluster_labels\n",
        "})\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(RESULTS_SAVE_PATH, index=False)\n",
        "print(f\"\\nResults saved to {RESULTS_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Cluster-to-Class Mapping Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze which classes are assigned to which clusters\n",
        "cluster_class_mapping = pd.crosstab(results_df['cluster_id'], results_df['true_label'])\n",
        "print(\"Cluster-to-Class Mapping:\")\n",
        "print(cluster_class_mapping)\n",
        "\n",
        "# Visualize cluster-class mapping\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(cluster_class_mapping, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=CLASS_NAMES, yticklabels=[f'Cluster {i}' for i in range(N_CLUSTERS)],\n",
        "            cbar_kws={'label': 'Number of Samples'})\n",
        "plt.title('Cluster-to-Class Mapping', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('True Class', fontsize=12)\n",
        "plt.ylabel('Cluster ID', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/cluster_class_mapping.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Create confusion matrix comparing clusters to true labels\n",
        "# Map each cluster to the most common class in that cluster\n",
        "cluster_to_class = {}\n",
        "for cluster_id in range(N_CLUSTERS):\n",
        "    cluster_data = results_df[results_df['cluster_id'] == cluster_id]\n",
        "    if len(cluster_data) > 0:\n",
        "        most_common_class = cluster_data['true_label'].mode()[0]\n",
        "        cluster_to_class[cluster_id] = most_common_class\n",
        "    else:\n",
        "        cluster_to_class[cluster_id] = CLASS_NAMES[0]\n",
        "\n",
        "# Map cluster labels to predicted class labels\n",
        "predicted_labels = np.array([cluster_to_class[cluster_id] for cluster_id in cluster_labels])\n",
        "\n",
        "# Define y_true and y_pred for evaluation (convert string labels to numeric indices)\n",
        "label_to_num = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
        "y_true = np.array([label_to_num[label] for label in train_labels])\n",
        "y_pred = np.array([label_to_num[label] for label in predicted_labels])\n",
        "\n",
        "# Calculate and print overall accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nOverall Accuracy Score: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - K-Means Clustering', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label (from Cluster)', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix as table\n",
        "print(\"\\nConfusion Matrix (Cluster assignments mapped to most common class):\")\n",
        "print(f\"{'':<15}\", end='')\n",
        "for name in CLASS_NAMES:\n",
        "    print(f\"{name:<15}\", end='')\n",
        "print()\n",
        "for i, name in enumerate(CLASS_NAMES):\n",
        "    print(f\"{name:<15}\", end='')\n",
        "    for j in range(len(CLASS_NAMES)):\n",
        "        print(f\"{cm[i][j]:<15}\", end='')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate silhouette score\n",
        "silhouette_avg = silhouette_score(X_features, cluster_labels)\n",
        "print(f\"Average Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(\"  (Range: -1 to 1, higher is better)\")\n",
        "\n",
        "# Calculate silhouette scores for each sample\n",
        "sample_silhouette_values = silhouette_samples(X_features, cluster_labels)\n",
        "\n",
        "# Calculate Adjusted Rand Index (if we want to compare with true labels)\n",
        "# Convert true labels to numeric\n",
        "label_to_num = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
        "true_label_numeric = np.array([label_to_num[label] for label in train_labels])\n",
        "ari_score = adjusted_rand_score(true_label_numeric, cluster_labels)\n",
        "print(f\"\\nAdjusted Rand Index: {ari_score:.4f}\")\n",
        "print(\"  (Range: -1 to 1, 1 = perfect match, 0 = random)\")\n",
        "\n",
        "# Print evaluation summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Inertia: {kmeans.inertia_:.2f}\")\n",
        "print(f\"Average Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {ari_score:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Silhouette Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize silhouette scores\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "y_lower = 10\n",
        "\n",
        "for i in range(N_CLUSTERS):\n",
        "    # Aggregate silhouette scores for samples belonging to cluster i\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    \n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    \n",
        "    color = plt.cm.nipy_spectral(float(i) / N_CLUSTERS)\n",
        "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    \n",
        "    # Label the silhouette plots with their cluster numbers at the middle\n",
        "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    \n",
        "    # Compute the new y_lower for next plot\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "ax.set_xlabel('Silhouette Coefficient Values', fontsize=12)\n",
        "ax.set_ylabel('Cluster Label', fontsize=12)\n",
        "ax.set_title('Silhouette Analysis for K-Means Clustering', fontsize=14, fontweight='bold')\n",
        "ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", \n",
        "           label=f'Average Score: {silhouette_avg:.4f}')\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim([-0.1, 1])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/silhouette_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 PCA Visualization (2D)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce to 2D for visualization using PCA\n",
        "pca_2d = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "X_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Clusters\n",
        "scatter1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, \n",
        "                          cmap='viridis', alpha=0.6, s=20)\n",
        "axes[0].set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)\n",
        "axes[0].set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)\n",
        "axes[0].set_title('K-Means Clustering Results (2D PCA)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')\n",
        "\n",
        "# Plot 2: True Labels\n",
        "label_colors = {CLASS_NAMES[0]: 0, CLASS_NAMES[1]: 1, CLASS_NAMES[2]: 2, CLASS_NAMES[3]: 3}\n",
        "label_numeric = np.array([label_colors[label] for label in train_labels])\n",
        "scatter2 = axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=label_numeric, \n",
        "                          cmap='viridis', alpha=0.6, s=20)\n",
        "axes[1].set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)\n",
        "axes[1].set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)\n",
        "axes[1].set_title('True Labels (2D PCA)', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Class ID')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/pca_2d_visualization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Cluster Centers Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cluster centers\n",
        "if USE_FEATURE_EXTRACTION and pca is not None:\n",
        "    # Transform cluster centers back to original space\n",
        "    cluster_centers_original = pca.inverse_transform(cluster_centers)\n",
        "    cluster_centers_original = scaler.inverse_transform(cluster_centers_original)\n",
        "else:\n",
        "    cluster_centers_original = scaler.inverse_transform(cluster_centers)\n",
        "\n",
        "# Reshape to image format\n",
        "cluster_centers_images = cluster_centers_original.reshape(N_CLUSTERS, IMG_SIZE[0], IMG_SIZE[1])\n",
        "\n",
        "# Visualize cluster centers\n",
        "fig, axes = plt.subplots(1, N_CLUSTERS, figsize=(16, 4))\n",
        "for i in range(N_CLUSTERS):\n",
        "    axes[i].imshow(cluster_centers_images[i], cmap='gray')\n",
        "    axes[i].set_title(f'Cluster {i} Center', fontsize=12, fontweight='bold')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('K-Means Cluster Centers (Averaged Images)', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/cluster_centers.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Sample Images from Each Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample images from each cluster\n",
        "n_samples_per_cluster = 5\n",
        "fig, axes = plt.subplots(N_CLUSTERS, n_samples_per_cluster, figsize=(15, 12))\n",
        "\n",
        "for cluster_id in range(N_CLUSTERS):\n",
        "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "    if len(cluster_indices) > 0:\n",
        "        # Randomly sample images from this cluster\n",
        "        sample_indices = np.random.choice(cluster_indices, \n",
        "                                         size=min(n_samples_per_cluster, len(cluster_indices)), \n",
        "                                         replace=False)\n",
        "        \n",
        "        for idx, sample_idx in enumerate(sample_indices):\n",
        "            img = train_images[sample_idx]\n",
        "            true_label = train_labels[sample_idx]\n",
        "            \n",
        "            axes[cluster_id, idx].imshow(img, cmap='gray')\n",
        "            axes[cluster_id, idx].set_title(f'True: {true_label}', fontsize=9)\n",
        "            axes[cluster_id, idx].axis('off')\n",
        "    else:\n",
        "        # Empty cluster\n",
        "        for idx in range(n_samples_per_cluster):\n",
        "            axes[cluster_id, idx].axis('off')\n",
        "            axes[cluster_id, idx].text(0.5, 0.5, 'No samples', \n",
        "                                      ha='center', va='center', fontsize=10)\n",
        "\n",
        "plt.suptitle('Sample Images from Each Cluster', fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/sample_images_by_cluster.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Elbow Method (Optimal K Selection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different values of K to find optimal number of clusters\n",
        "print(\"Testing different values of K...\")\n",
        "K_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in tqdm(K_range):\n",
        "    kmeans_test = KMeans(n_clusters=k, max_iter=MAX_ITER, n_init=5, \n",
        "                        random_state=RANDOM_STATE, verbose=0)\n",
        "    kmeans_test.fit(X_features)\n",
        "    inertias.append(kmeans_test.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_features, kmeans_test.labels_))\n",
        "\n",
        "# Plot elbow curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Inertia plot (Elbow method)\n",
        "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "axes[0].set_ylabel('Inertia', fontsize=12)\n",
        "axes[0].set_title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axvline(x=N_CLUSTERS, color='r', linestyle='--', \n",
        "                label=f'Selected K={N_CLUSTERS}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Silhouette score plot\n",
        "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
        "axes[1].set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axvline(x=N_CLUSTERS, color='r', linestyle='--', \n",
        "                label=f'Selected K={N_CLUSTERS}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATION_DIR}/elbow_method.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOptimal K (highest silhouette score): {K_range[np.argmax(silhouette_scores)]}\")\n",
        "print(f\"Silhouette scores: {dict(zip(K_range, silhouette_scores))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the K-Means model and preprocessing objects\n",
        "model_data = {\n",
        "    'kmeans': kmeans,\n",
        "    'scaler': scaler,\n",
        "    'pca': pca,\n",
        "    'n_clusters': N_CLUSTERS,\n",
        "    'img_size': IMG_SIZE,\n",
        "    'use_feature_extraction': USE_FEATURE_EXTRACTION,\n",
        "    'n_components': N_COMPONENTS if USE_FEATURE_EXTRACTION else None\n",
        "}\n",
        "\n",
        "with open(MODEL_SAVE_PATH, 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
        "print(\"\\nSaved components:\")\n",
        "print(\"  - K-Means model\")\n",
        "print(\"  - StandardScaler\")\n",
        "if USE_FEATURE_EXTRACTION:\n",
        "    print(\"  - PCA transformer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "### Clustering Results Summary:\n",
        "- **Number of Clusters**: 4 (configurable via N_CLUSTERS)\n",
        "- **Inertia**: Calculated above\n",
        "- **Average Silhouette Score**: Calculated above\n",
        "- **Adjusted Rand Index**: Calculated above\n",
        "- **Model saved to**: `models/kmeans/kmeans_clusterer.pkl`\n",
        "- **Results saved to**: `models/kmeans/kmeans_results.csv`\n",
        "\n",
        "### Files Generated:\n",
        "1. Trained model: `models/kmeans/kmeans_clusterer.pkl`\n",
        "2. Cluster assignments: `models/kmeans/kmeans_results.csv`\n",
        "3. Class distribution: `models/kmeans/visualizations/class_distribution.png`\n",
        "4. Cluster-class mapping: `models/kmeans/visualizations/cluster_class_mapping.png`\n",
        "5. Confusion matrix: `models/kmeans/visualizations/confusion_matrix.png`\n",
        "6. Silhouette analysis: `models/kmeans/visualizations/silhouette_analysis.png`\n",
        "7. PCA 2D visualization: `models/kmeans/visualizations/pca_2d_visualization.png`\n",
        "8. Cluster centers: `models/kmeans/visualizations/cluster_centers.png`\n",
        "9. Sample images by cluster: `models/kmeans/visualizations/sample_images_by_cluster.png`\n",
        "10. Elbow method analysis: `models/kmeans/visualizations/elbow_method.png`\n",
        "\n",
        "### Using the Trained Model:\n",
        "\n",
        "**Load and use the model:**\n",
        "```python\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "with open('models/kmeans/kmeans_clusterer.pkl', 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "kmeans = model_data['kmeans']\n",
        "scaler = model_data['scaler']\n",
        "pca = model_data['pca']\n",
        "\n",
        "# Predict cluster for a new image\n",
        "def predict_cluster(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, model_data['img_size'])\n",
        "    img_flattened = img.reshape(1, -1)\n",
        "    img_scaled = scaler.transform(img_flattened)\n",
        "    \n",
        "    if model_data['use_feature_extraction']:\n",
        "        img_features = pca.transform(img_scaled)\n",
        "    else:\n",
        "        img_features = img_scaled\n",
        "    \n",
        "    cluster_id = kmeans.predict(img_features)[0]\n",
        "    return cluster_id\n",
        "\n",
        "# Example usage\n",
        "cluster = predict_cluster('path/to/image.jpg')\n",
        "print(f'Image belongs to cluster: {cluster}')\n",
        "```\n",
        "\n",
        "### Important Notes:\n",
        "- K-Means is an unsupervised algorithm - it doesn't use class labels during training\n",
        "- Cluster assignments may not perfectly match true class labels\n",
        "- The Adjusted Rand Index shows how well clusters align with true classes\n",
        "- You can adjust `N_CLUSTERS` to experiment with different numbers of clusters\n",
        "- Use the elbow method plot to find the optimal number of clusters\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep-mric",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
