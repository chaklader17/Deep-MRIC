{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MobileNet-V2 Training for Brain Tumor Classification\n",
        "\n",
        "This notebook trains a MobileNet-V2 model from scratch to classify brain MRI images into 4 categories:\n",
        "- **NO_TUMOR**: Healthy brain (no tumor detected)\n",
        "- **GLIOMA**: Glioma tumor type\n",
        "- **MENINGIOMA**: Meningioma tumor type\n",
        "- **PITUITARY**: Pituitary tumor type\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook includes:\n",
        "1. Data loading and preprocessing (using same pipeline as VGG16 preprocessing)\n",
        "2. MobileNet-V2 architecture implementation from scratch with Depthwise Separable Convolutions and Inverted Residual Blocks\n",
        "3. Training pipeline with validation\n",
        "4. Model saving based on best validation accuracy\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Make sure you have installed all required packages:\n",
        "```bash\n",
        "pip install torch torchvision seaborn pandas scikit-learn matplotlib numpy tqdm\n",
        "```\n",
        "\n",
        "Or install from requirements.txt:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = 'data/vgg16_classification'\n",
        "MODEL_SAVE_PATH = 'mobilenet_model.pth'\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = ['NO_TUMOR', 'GLIOMA', 'MENINGIOMA', 'PITUITARY']\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"  Model save path: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Dataset Class (from preprocess_vgg16 pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset class that uses CSV metadata to prevent data leakage\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class FilteredImageFolder(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset that loads images based on CSV metadata to prevent data leakage.\n",
        "    Only loads images whose original_filename is NOT in other splits.\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata_df, split_name, transform=None, base_dir='data/vgg16_classification'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_df: DataFrame with columns: image_path, full_path, class, split, filename, original_filename (optional)\n",
        "            split_name: 'train', 'val', or 'test'\n",
        "            transform: Image transforms\n",
        "            base_dir: Base directory for images\n",
        "        \"\"\"\n",
        "        self.split_name = split_name\n",
        "        self.transform = transform\n",
        "        self.base_dir = base_dir\n",
        "        \n",
        "        # Filter by split\n",
        "        split_df = metadata_df[metadata_df['split'] == split_name].copy()\n",
        "        \n",
        "        # For train: use augmented metadata, filter by original_filename not in test/val\n",
        "        if split_name == 'train':\n",
        "            # Get original filenames from test and val splits (from original metadata)\n",
        "            if 'original_filename' in metadata_df.columns:\n",
        "                # This is augmented metadata\n",
        "                orig_metadata_path = 'data/dataset_metadata.csv'\n",
        "                if os.path.exists(orig_metadata_path):\n",
        "                    orig_df = pd.read_csv(orig_metadata_path)\n",
        "                    test_originals = set(orig_df[orig_df['split'] == 'test']['filename'].unique())\n",
        "                    val_originals = set(orig_df[orig_df['split'] == 'val']['filename'].unique())\n",
        "                    excluded_originals = test_originals.union(val_originals)\n",
        "                    # Filter: only keep images whose original_filename is NOT in test/val\n",
        "                    split_df = split_df[~split_df['original_filename'].isin(excluded_originals)]\n",
        "        \n",
        "        # For val/test: use original metadata, ensure no overlap with train\n",
        "        elif split_name in ['val', 'test']:\n",
        "            # Get train original filenames (from augmented metadata if available)\n",
        "            aug_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "            if os.path.exists(aug_metadata_path):\n",
        "                aug_df = pd.read_csv(aug_metadata_path)\n",
        "                train_originals = set(aug_df[aug_df['split'] == 'train']['original_filename'].unique())\n",
        "                # Filter: only keep images whose filename is NOT in train\n",
        "                split_df = split_df[~split_df['filename'].isin(train_originals)]\n",
        "        \n",
        "        self.samples = []\n",
        "        self.classes = sorted(split_df['class'].unique().tolist())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        \n",
        "        for _, row in split_df.iterrows():\n",
        "            # Use full_path if available, otherwise construct from base_dir and image_path\n",
        "            if pd.notna(row.get('full_path')):\n",
        "                img_path = row['full_path']\n",
        "            else:\n",
        "                img_path = os.path.join(base_dir, row['image_path'])\n",
        "            \n",
        "            # Normalize path separators\n",
        "            img_path = img_path.replace('\\\\', '/')\n",
        "            \n",
        "            if os.path.exists(img_path):\n",
        "                label = self.class_to_idx[row['class']]\n",
        "                self.samples.append((img_path, label))\n",
        "            else:\n",
        "                print(f\"Warning: Image not found: {img_path}\")\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} images for {split_name} split (filtered from {len(split_df)} rows in CSV)\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transforms (same as VGG16 preprocessing pipeline)\n",
        "# Training: only normalization (augmentation already applied via augment_training_data.py)\n",
        "# Use train_augmented directory which contains pre-augmented images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test: only normalization (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets using CSV metadata to prevent data leakage\n",
        "print(\"Loading datasets using CSV metadata files to prevent data leakage...\")\n",
        "\n",
        "# Load metadata CSV files\n",
        "augmented_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "original_metadata_path = 'data/dataset_metadata.csv'\n",
        "\n",
        "if os.path.exists(augmented_metadata_path):\n",
        "    aug_metadata_df = pd.read_csv(augmented_metadata_path)\n",
        "    print(f\"Loaded augmented metadata: {len(aug_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {augmented_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    aug_metadata_df = pd.DataFrame()\n",
        "\n",
        "if os.path.exists(original_metadata_path):\n",
        "    orig_metadata_df = pd.read_csv(original_metadata_path)\n",
        "    print(f\"Loaded original metadata: {len(orig_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {original_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    orig_metadata_df = pd.DataFrame()\n",
        "\n",
        "# Use FilteredImageFolder for train (from augmented metadata)\n",
        "if len(aug_metadata_df) > 0:\n",
        "    train_dataset = FilteredImageFolder(aug_metadata_df, 'train', transform=train_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for train (no augmented metadata found)\")\n",
        "    train_dir = os.path.join(DATA_DIR, 'train_augmented')\n",
        "    if not os.path.exists(train_dir):\n",
        "        train_dir = os.path.join(DATA_DIR, 'train')\n",
        "    train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "\n",
        "# Use FilteredImageFolder for val and test (from original metadata)\n",
        "if len(orig_metadata_df) > 0:\n",
        "    val_dataset = FilteredImageFolder(orig_metadata_df, 'val', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "    test_dataset = FilteredImageFolder(orig_metadata_df, 'test', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for val/test (no original metadata found)\")\n",
        "    val_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'val'), transform=val_test_transform)\n",
        "    test_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'test'), transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'\\nTrain samples: {len(train_dataset)}')\n",
        "print(f'Validation samples: {len(val_dataset)}')\n",
        "print(f'Test samples: {len(test_dataset)}')\n",
        "print(f'Number of classes: {len(train_dataset.classes)}')\n",
        "print(f'Class names: {train_dataset.classes}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MobileNet-V2 Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvBNReLU(nn.Sequential):\n",
        "    \"\"\"Convolution + Batch Normalization + ReLU6 activation\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    \"\"\"\n",
        "    Inverted Residual Block (MobileNet-V2 building block)\n",
        "    \n",
        "    Structure:\n",
        "    1. 1x1 pointwise expansion (expand channels)\n",
        "    2. 3x3 depthwise separable convolution\n",
        "    3. 1x1 pointwise projection (squeeze channels)\n",
        "    4. Residual connection (if stride=1 and input_channels == output_channels)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride, expand_ratio=6):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "        \n",
        "        hidden_dim = int(round(in_channels * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n",
        "        \n",
        "        layers = []\n",
        "        # Expansion phase: 1x1 pointwise convolution\n",
        "        if expand_ratio != 1:\n",
        "            layers.append(ConvBNReLU(in_channels, hidden_dim, kernel_size=1))\n",
        "        \n",
        "        # Depthwise phase: 3x3 depthwise separable convolution\n",
        "        layers.append(ConvBNReLU(hidden_dim, hidden_dim, kernel_size=3, stride=stride, groups=hidden_dim))\n",
        "        \n",
        "        # Projection phase: 1x1 pointwise convolution (linear, no activation)\n",
        "        layers.append(nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "        \n",
        "        self.conv = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    \"\"\"\n",
        "    MobileNet-V2 architecture from scratch\n",
        "    \n",
        "    Key features:\n",
        "    - Depthwise Separable Convolutions\n",
        "    - Inverted Residual Blocks with expansion and squeeze\n",
        "    - Batch Normalization after all convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, width_mult=1.0):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        \n",
        "        # First layer: standard convolution\n",
        "        input_channel = int(32 * width_mult)\n",
        "        self.first_conv = ConvBNReLU(3, input_channel, kernel_size=3, stride=2)\n",
        "        \n",
        "        # Inverted residual blocks configuration\n",
        "        # (t, c, n, s) where:\n",
        "        # t: expansion ratio\n",
        "        # c: output channels\n",
        "        # n: number of blocks\n",
        "        # s: stride for first block\n",
        "        inverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "        \n",
        "        # Build inverted residual blocks\n",
        "        features = []\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = int(c * width_mult)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(InvertedResidual(input_channel, output_channel, stride, expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "        \n",
        "        self.features = nn.Sequential(*features)\n",
        "        \n",
        "        # Last few layers\n",
        "        last_channel = int(1280 * width_mult) if width_mult > 1.0 else 1280\n",
        "        self.last_conv = ConvBNReLU(input_channel, last_channel, kernel_size=1)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(last_channel, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.first_conv(x)\n",
        "        x = self.features(x)\n",
        "        x = self.last_conv(x)\n",
        "        x = x.mean([2, 3])  # Global average pooling\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = MobileNetV2(num_classes=NUM_CLASSES, width_mult=1.0).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"MobileNet-V2 Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam (as required)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler: Cosine Annealing (better for training from scratch)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "\n",
        "print(f\"Loss function: CrossEntropyLoss\")\n",
        "print(f\"Optimizer: Adam (lr={LEARNING_RATE}, weight_decay=1e-5)\")\n",
        "print(f\"Learning rate scheduler: CosineAnnealingLR (T_max={NUM_EPOCHS}, eta_min=1e-6)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(loader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Validating'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training for {NUM_EPOCHS} epochs\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model (based on validation accuracy)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'history': history\n",
        "        }, MODEL_SAVE_PATH)\n",
        "        print(f\"Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Curves Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curve\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curve\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('mobilenet_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Load Best Model and Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print overall accuracy score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - MobileNet-V2 Model', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('mobilenet_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and print the full classification report\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Test Accuracy**: Calculated above\n",
        "- **Best Validation Accuracy**: Calculated above\n",
        "- **Model saved to**: `mobilenet_model.pth`\n",
        "\n",
        "### Architecture Features:\n",
        "- Full MobileNet-V2 implementation from scratch\n",
        "- Depthwise Separable Convolutions for efficiency\n",
        "- Inverted Residual Blocks with expansion and squeeze phases\n",
        "- Batch Normalization after all convolutional layers\n",
        "- Adam Optimizer with Cosine Annealing Learning Rate Scheduler\n",
        "\n",
        "### Files Generated:\n",
        "1. Trained model: `mobilenet_model.pth`\n",
        "2. Training curves: `mobilenet_training_curves.png`\n",
        "3. Confusion matrix: `mobilenet_confusion_matrix.png`\n",
        "\n",
        "All files are saved in the current directory.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
