{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inception-V1 (GoogLeNet) Training for Brain Tumor Classification\n",
        "\n",
        "This notebook trains an Inception-V1 (GoogLeNet) model from scratch to classify brain MRI images into 4 categories:\n",
        "- **NO_TUMOR**: Healthy brain (no tumor detected)\n",
        "- **GLIOMA**: Glioma tumor type\n",
        "- **MENINGIOMA**: Meningioma tumor type\n",
        "- **PITUITARY**: Pituitary tumor type\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook includes:\n",
        "1. Data loading and preprocessing (using same pipeline as VGG16 preprocessing)\n",
        "2. Inception-V1 (GoogLeNet) architecture implementation from scratch\n",
        "3. Training pipeline with validation\n",
        "4. Model saving based on best validation accuracy\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Make sure you have installed all required packages:\n",
        "```bash\n",
        "pip install torch torchvision seaborn pandas scikit-learn matplotlib numpy tqdm\n",
        "```\n",
        "\n",
        "Or install from requirements.txt:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = 'data/vgg16_classification'\n",
        "MODEL_SAVE_PATH = 'inception_model.pth'\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = ['NO_TUMOR', 'GLIOMA', 'MENINGIOMA', 'PITUITARY']\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"  Model save path: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Dataset Class (from preprocess_vgg16 pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset class that uses CSV metadata to prevent data leakage\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class FilteredImageFolder(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset that loads images based on CSV metadata to prevent data leakage.\n",
        "    Only loads images whose original_filename is NOT in other splits.\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata_df, split_name, transform=None, base_dir='data/vgg16_classification'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_df: DataFrame with columns: image_path, full_path, class, split, filename, original_filename (optional)\n",
        "            split_name: 'train', 'val', or 'test'\n",
        "            transform: Image transforms\n",
        "            base_dir: Base directory for images\n",
        "        \"\"\"\n",
        "        self.split_name = split_name\n",
        "        self.transform = transform\n",
        "        self.base_dir = base_dir\n",
        "        \n",
        "        # Filter by split\n",
        "        split_df = metadata_df[metadata_df['split'] == split_name].copy()\n",
        "        \n",
        "        # For train: use augmented metadata, filter by original_filename not in test/val\n",
        "        if split_name == 'train':\n",
        "            # Get original filenames from test and val splits (from original metadata)\n",
        "            if 'original_filename' in metadata_df.columns:\n",
        "                # This is augmented metadata\n",
        "                orig_metadata_path = 'data/dataset_metadata.csv'\n",
        "                if os.path.exists(orig_metadata_path):\n",
        "                    orig_df = pd.read_csv(orig_metadata_path)\n",
        "                    test_originals = set(orig_df[orig_df['split'] == 'test']['filename'].unique())\n",
        "                    val_originals = set(orig_df[orig_df['split'] == 'val']['filename'].unique())\n",
        "                    excluded_originals = test_originals.union(val_originals)\n",
        "                    # Filter: only keep images whose original_filename is NOT in test/val\n",
        "                    split_df = split_df[~split_df['original_filename'].isin(excluded_originals)]\n",
        "        \n",
        "        # For val/test: use original metadata, ensure no overlap with train\n",
        "        elif split_name in ['val', 'test']:\n",
        "            # Get train original filenames (from augmented metadata if available)\n",
        "            aug_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "            if os.path.exists(aug_metadata_path):\n",
        "                aug_df = pd.read_csv(aug_metadata_path)\n",
        "                train_originals = set(aug_df[aug_df['split'] == 'train']['original_filename'].unique())\n",
        "                # Filter: only keep images whose filename is NOT in train\n",
        "                split_df = split_df[~split_df['filename'].isin(train_originals)]\n",
        "        \n",
        "        self.samples = []\n",
        "        self.classes = sorted(split_df['class'].unique().tolist())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        \n",
        "        for _, row in split_df.iterrows():\n",
        "            # Use full_path if available, otherwise construct from base_dir and image_path\n",
        "            if pd.notna(row.get('full_path')):\n",
        "                img_path = row['full_path']\n",
        "            else:\n",
        "                img_path = os.path.join(base_dir, row['image_path'])\n",
        "            \n",
        "            # Normalize path separators\n",
        "            img_path = img_path.replace('\\\\', '/')\n",
        "            \n",
        "            if os.path.exists(img_path):\n",
        "                label = self.class_to_idx[row['class']]\n",
        "                self.samples.append((img_path, label))\n",
        "            else:\n",
        "                print(f\"Warning: Image not found: {img_path}\")\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} images for {split_name} split (filtered from {len(split_df)} rows in CSV)\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transforms (same as VGG16 preprocessing pipeline)\n",
        "# Training: only normalization (augmentation already applied via augment_training_data.py)\n",
        "# Use train_augmented directory which contains pre-augmented images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test: only normalization (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets using CSV metadata to prevent data leakage\n",
        "print(\"Loading datasets using CSV metadata files to prevent data leakage...\")\n",
        "\n",
        "# Load metadata CSV files\n",
        "augmented_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "original_metadata_path = 'data/dataset_metadata.csv'\n",
        "\n",
        "if os.path.exists(augmented_metadata_path):\n",
        "    aug_metadata_df = pd.read_csv(augmented_metadata_path)\n",
        "    print(f\"Loaded augmented metadata: {len(aug_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {augmented_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    aug_metadata_df = pd.DataFrame()\n",
        "\n",
        "if os.path.exists(original_metadata_path):\n",
        "    orig_metadata_df = pd.read_csv(original_metadata_path)\n",
        "    print(f\"Loaded original metadata: {len(orig_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {original_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    orig_metadata_df = pd.DataFrame()\n",
        "\n",
        "# Use FilteredImageFolder for train (from augmented metadata)\n",
        "if len(aug_metadata_df) > 0:\n",
        "    train_dataset = FilteredImageFolder(aug_metadata_df, 'train', transform=train_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for train (no augmented metadata found)\")\n",
        "    train_dir = os.path.join(DATA_DIR, 'train_augmented')\n",
        "    if not os.path.exists(train_dir):\n",
        "        train_dir = os.path.join(DATA_DIR, 'train')\n",
        "    train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "\n",
        "# Use FilteredImageFolder for val and test (from original metadata)\n",
        "if len(orig_metadata_df) > 0:\n",
        "    val_dataset = FilteredImageFolder(orig_metadata_df, 'val', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "    test_dataset = FilteredImageFolder(orig_metadata_df, 'test', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for val/test (no original metadata found)\")\n",
        "    val_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'val'), transform=val_test_transform)\n",
        "    test_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'test'), transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'\\nTrain samples: {len(train_dataset)}')\n",
        "print(f'Validation samples: {len(val_dataset)}')\n",
        "print(f'Test samples: {len(test_dataset)}')\n",
        "print(f'Number of classes: {len(train_dataset.classes)}')\n",
        "print(f'Class names: {train_dataset.classes}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inception-V1 (GoogLeNet) Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicConv2d(nn.Module):\n",
        "    \"\"\"Basic convolutional block with Batch Normalization and ReLU\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Inception block with multiple parallel paths:\n",
        "    1. 1x1 convolution\n",
        "    2. 1x1 -> 3x3 convolution\n",
        "    3. 1x1 -> 5x5 convolution\n",
        "    4. 3x3 max pooling -> 1x1 convolution\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        \n",
        "        # Branch 1: 1x1 convolution\n",
        "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
        "        \n",
        "        # Branch 2: 1x1 -> 3x3 convolution\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
        "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "        \n",
        "        # Branch 3: 1x1 -> 5x5 convolution\n",
        "        self.branch3 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
        "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "        \n",
        "        # Branch 4: 3x3 max pooling -> 1x1 convolution\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        \n",
        "        # Concatenate all branches along channel dimension\n",
        "        outputs = [branch1, branch2, branch3, branch4]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class AuxiliaryClassifier(nn.Module):\n",
        "    \"\"\"Auxiliary classifier for training stability (used during training only)\"\"\"\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(AuxiliaryClassifier, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 1024)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionV1(nn.Module):\n",
        "    \"\"\"\n",
        "    Inception-V1 (GoogLeNet) architecture from scratch\n",
        "    \n",
        "    Architecture:\n",
        "    - Initial convolutional layers\n",
        "    - Multiple Inception blocks\n",
        "    - Auxiliary classifiers (for training stability)\n",
        "    - Final classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, aux_logits=True):\n",
        "        super(InceptionV1, self).__init__()\n",
        "        self.aux_logits = aux_logits\n",
        "        \n",
        "        # Initial convolutional layers\n",
        "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
        "        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # Inception blocks (3a, 3b)\n",
        "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # Inception blocks (4a, 4b, 4c, 4d, 4e)\n",
        "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # Inception blocks (5a, 5b)\n",
        "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
        "        \n",
        "        # Auxiliary classifiers (for training stability)\n",
        "        if aux_logits:\n",
        "            self.aux1 = AuxiliaryClassifier(512, num_classes)  # After 4a\n",
        "            self.aux2 = AuxiliaryClassifier(528, num_classes)  # After 4d\n",
        "        \n",
        "        # Final classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initial convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool2(x)\n",
        "        \n",
        "        # Inception blocks 3a, 3b\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "        \n",
        "        # Inception blocks 4a, 4b, 4c, 4d, 4e\n",
        "        x = self.inception4a(x)\n",
        "        aux1 = None\n",
        "        if self.aux_logits and self.training:\n",
        "            aux1 = self.aux1(x)\n",
        "        \n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        aux2 = None\n",
        "        if self.aux_logits and self.training:\n",
        "            aux2 = self.aux2(x)\n",
        "        \n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "        \n",
        "        # Inception blocks 5a, 5b\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "        \n",
        "        # Final classifier\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        if self.aux_logits and self.training:\n",
        "            return x, aux1, aux2\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = InceptionV1(num_classes=NUM_CLASSES, aux_logits=True).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"Inception-V1 (GoogLeNet) Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam (as required)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler: Cosine Annealing (better for training from scratch)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "\n",
        "print(f\"Loss function: CrossEntropyLoss\")\n",
        "print(f\"Optimizer: Adam (lr={LEARNING_RATE}, weight_decay=1e-5)\")\n",
        "print(f\"Learning rate scheduler: CosineAnnealingLR (T_max={NUM_EPOCHS}, eta_min=1e-6)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(loader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Inception outputs main prediction and auxiliary predictions during training\n",
        "        if model.aux_logits:\n",
        "            outputs, aux1, aux2 = model(images)\n",
        "            loss1 = criterion(outputs, labels)\n",
        "            loss2 = criterion(aux1, labels)\n",
        "            loss3 = criterion(aux2, labels)\n",
        "            loss = loss1 + 0.3 * loss2 + 0.3 * loss3  # Weighted loss with auxiliary classifiers\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Validating'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            # During validation, auxiliary classifiers are not used\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training for {NUM_EPOCHS} epochs\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model (based on validation accuracy)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        # Disable auxiliary classifiers for inference\n",
        "        model.aux_logits = False\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'history': history\n",
        "        }, MODEL_SAVE_PATH)\n",
        "        model.aux_logits = True  # Re-enable for training\n",
        "        print(f\"Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Curves Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curve\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curve\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('inception_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Load Best Model and Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.aux_logits = False  # Disable auxiliary classifiers for inference\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print overall accuracy score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - Inception-V1 Model', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('inception_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and print the full classification report\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Test Accuracy**: Calculated above\n",
        "- **Best Validation Accuracy**: Calculated above\n",
        "- **Model saved to**: `inception_model.pth`\n",
        "\n",
        "### Architecture Features:\n",
        "- Full Inception-V1 (GoogLeNet) implementation from scratch\n",
        "- Inception blocks with parallel convolutions (1x1, 3x3, 5x5, and pooling)\n",
        "- Batch Normalization after all convolutional layers\n",
        "- Auxiliary classifiers for training stability\n",
        "- Adam Optimizer with Cosine Annealing Learning Rate Scheduler\n",
        "\n",
        "### Files Generated:\n",
        "1. Trained model: `inception_model.pth`\n",
        "2. Training curves: `inception_training_curves.png`\n",
        "3. Confusion matrix: `inception_confusion_matrix.png`\n",
        "\n",
        "All files are saved in the current directory.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
