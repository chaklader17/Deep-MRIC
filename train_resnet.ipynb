{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-18 Training for Brain Tumor Classification\n",
        "\n",
        "This notebook trains a ResNet-18 model from scratch to classify brain MRI images into 4 categories:\n",
        "- **NO_TUMOR**: Healthy brain (no tumor detected)\n",
        "- **GLIOMA**: Glioma tumor type\n",
        "- **MENINGIOMA**: Meningioma tumor type\n",
        "- **PITUITARY**: Pituitary tumor type\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook includes:\n",
        "1. Data loading and preprocessing (using same pipeline as VGG16 preprocessing)\n",
        "2. ResNet-18 architecture implementation from scratch with BasicBlock and skip connections\n",
        "3. Training pipeline with validation\n",
        "4. Model saving based on best validation accuracy\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Make sure you have installed all required packages:\n",
        "```bash\n",
        "pip install torch torchvision seaborn pandas scikit-learn matplotlib numpy tqdm\n",
        "```\n",
        "\n",
        "Or install from requirements.txt:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = 'data/vgg16_classification'\n",
        "MODEL_SAVE_PATH = 'resnet_model.pth'\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = ['NO_TUMOR', 'GLIOMA', 'MENINGIOMA', 'PITUITARY']\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"  Model save path: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Dataset Class (from preprocess_vgg16 pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset class that uses CSV metadata to prevent data leakage\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class FilteredImageFolder(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset that loads images based on CSV metadata to prevent data leakage.\n",
        "    Only loads images whose original_filename is NOT in other splits.\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata_df, split_name, transform=None, base_dir='data/vgg16_classification'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_df: DataFrame with columns: image_path, full_path, class, split, filename, original_filename (optional)\n",
        "            split_name: 'train', 'val', or 'test'\n",
        "            transform: Image transforms\n",
        "            base_dir: Base directory for images\n",
        "        \"\"\"\n",
        "        self.split_name = split_name\n",
        "        self.transform = transform\n",
        "        self.base_dir = base_dir\n",
        "        \n",
        "        # Filter by split\n",
        "        split_df = metadata_df[metadata_df['split'] == split_name].copy()\n",
        "        \n",
        "        # For train: use augmented metadata, filter by original_filename not in test/val\n",
        "        if split_name == 'train':\n",
        "            # Get original filenames from test and val splits (from original metadata)\n",
        "            if 'original_filename' in metadata_df.columns:\n",
        "                # This is augmented metadata\n",
        "                orig_metadata_path = 'data/dataset_metadata.csv'\n",
        "                if os.path.exists(orig_metadata_path):\n",
        "                    orig_df = pd.read_csv(orig_metadata_path)\n",
        "                    test_originals = set(orig_df[orig_df['split'] == 'test']['filename'].unique())\n",
        "                    val_originals = set(orig_df[orig_df['split'] == 'val']['filename'].unique())\n",
        "                    excluded_originals = test_originals.union(val_originals)\n",
        "                    # Filter: only keep images whose original_filename is NOT in test/val\n",
        "                    split_df = split_df[~split_df['original_filename'].isin(excluded_originals)]\n",
        "        \n",
        "        # For val/test: use original metadata, ensure no overlap with train\n",
        "        elif split_name in ['val', 'test']:\n",
        "            # Get train original filenames (from augmented metadata if available)\n",
        "            aug_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "            if os.path.exists(aug_metadata_path):\n",
        "                aug_df = pd.read_csv(aug_metadata_path)\n",
        "                train_originals = set(aug_df[aug_df['split'] == 'train']['original_filename'].unique())\n",
        "                # Filter: only keep images whose filename is NOT in train\n",
        "                split_df = split_df[~split_df['filename'].isin(train_originals)]\n",
        "        \n",
        "        self.samples = []\n",
        "        self.classes = sorted(split_df['class'].unique().tolist())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        \n",
        "        for _, row in split_df.iterrows():\n",
        "            # Use full_path if available, otherwise construct from base_dir and image_path\n",
        "            if pd.notna(row.get('full_path')):\n",
        "                img_path = row['full_path']\n",
        "            else:\n",
        "                img_path = os.path.join(base_dir, row['image_path'])\n",
        "            \n",
        "            # Normalize path separators\n",
        "            img_path = img_path.replace('\\\\', '/')\n",
        "            \n",
        "            if os.path.exists(img_path):\n",
        "                label = self.class_to_idx[row['class']]\n",
        "                self.samples.append((img_path, label))\n",
        "            else:\n",
        "                print(f\"Warning: Image not found: {img_path}\")\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} images for {split_name} split (filtered from {len(split_df)} rows in CSV)\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transforms (same as train_cnn.ipynb)\n",
        "# Training: only normalization (augmentation already applied via augment_training_data.py)\n",
        "# Use train_augmented directory which contains pre-augmented images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test: only normalization (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets using CSV metadata to prevent data leakage\n",
        "print(\"Loading datasets using CSV metadata files to prevent data leakage...\")\n",
        "\n",
        "# Load metadata CSV files\n",
        "augmented_metadata_path = 'data/augmented_dataset_metadata.csv'\n",
        "original_metadata_path = 'data/dataset_metadata.csv'\n",
        "\n",
        "if os.path.exists(augmented_metadata_path):\n",
        "    aug_metadata_df = pd.read_csv(augmented_metadata_path)\n",
        "    print(f\"Loaded augmented metadata: {len(aug_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {augmented_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    aug_metadata_df = pd.DataFrame()\n",
        "\n",
        "if os.path.exists(original_metadata_path):\n",
        "    orig_metadata_df = pd.read_csv(original_metadata_path)\n",
        "    print(f\"Loaded original metadata: {len(orig_metadata_df)} rows\")\n",
        "else:\n",
        "    print(f\"Warning: {original_metadata_path} not found. Creating empty DataFrame.\")\n",
        "    orig_metadata_df = pd.DataFrame()\n",
        "\n",
        "# Use FilteredImageFolder for train (from augmented metadata)\n",
        "if len(aug_metadata_df) > 0:\n",
        "    train_dataset = FilteredImageFolder(aug_metadata_df, 'train', transform=train_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for train (no augmented metadata found)\")\n",
        "    train_dir = os.path.join(DATA_DIR, 'train_augmented')\n",
        "    if not os.path.exists(train_dir):\n",
        "        train_dir = os.path.join(DATA_DIR, 'train')\n",
        "    train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "\n",
        "# Use FilteredImageFolder for val and test (from original metadata)\n",
        "if len(orig_metadata_df) > 0:\n",
        "    val_dataset = FilteredImageFolder(orig_metadata_df, 'val', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "    test_dataset = FilteredImageFolder(orig_metadata_df, 'test', transform=val_test_transform, base_dir=DATA_DIR)\n",
        "else:\n",
        "    print(\"Falling back to ImageFolder for val/test (no original metadata found)\")\n",
        "    val_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'val'), transform=val_test_transform)\n",
        "    test_dataset = ImageFolder(root=os.path.join(DATA_DIR, 'test'), transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'\\nTrain samples: {len(train_dataset)}')\n",
        "print(f'Validation samples: {len(val_dataset)}')\n",
        "print(f'Test samples: {len(test_dataset)}')\n",
        "print(f'Number of classes: {len(train_dataset.classes)}')\n",
        "print(f'Class names: {train_dataset.classes}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ResNet-18 Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    BasicBlock for ResNet-18 and ResNet-34\n",
        "    Contains two 3x3 convolutions with skip connection\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # First 3x3 convolution\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # Second 3x3 convolution\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # Downsample layer for skip connection when dimensions change\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        \n",
        "        # First convolution block\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        # Second convolution block\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        # Apply downsample to identity if needed (for dimension matching)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        \n",
        "        # Skip connection: add identity to output\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet-18 architecture from scratch\n",
        "    \n",
        "    Architecture:\n",
        "    - Initial 7x7 convolution + max pooling\n",
        "    - 4 layers, each with 2 BasicBlocks\n",
        "    - Global average pooling\n",
        "    - Fully connected classifier\n",
        "    \n",
        "    Uses He/Kaiming initialization for optimal training from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ResNet18, self).__init__()\n",
        "        \n",
        "        # Initial layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # ResNet layers\n",
        "        # Layer 1: 64 channels, 2 blocks\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        \n",
        "        # Layer 2: 64->128 channels, 2 blocks\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        \n",
        "        # Layer 3: 128->256 channels, 2 blocks\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        \n",
        "        # Layer 4: 256->512 channels, 2 blocks\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        \n",
        "        # Global average pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
        "        \n",
        "        # Initialize weights using He/Kaiming initialization\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        \"\"\"\n",
        "        Create a layer with multiple BasicBlocks\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        \n",
        "        # If stride != 1 or channels change, we need a downsample layer for skip connection\n",
        "        if stride != 1 or in_channels != out_channels * BasicBlock.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion,\n",
        "                         kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "        \n",
        "        layers = []\n",
        "        # First block may have stride > 1 and/or channel change\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride, downsample))\n",
        "        \n",
        "        # Remaining blocks have stride=1 and same channels\n",
        "        in_channels = out_channels * BasicBlock.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(in_channels, out_channels))\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize convolutional weights using He/Kaiming initialization\n",
        "        This is optimal for ReLU activations when training from scratch\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # He/Kaiming initialization for ReLU\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initial layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        # ResNet layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        # Global average pooling and classifier\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = ResNet18(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"ResNet-18 Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam (same as train_cnn.ipynb)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler: ReduceLROnPlateau (same as train_cnn.ipynb)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "print(f\"Loss function: CrossEntropyLoss\")\n",
        "print(f\"Optimizer: Adam (lr={LEARNING_RATE}, weight_decay=1e-5)\")\n",
        "print(f\"Learning rate scheduler: ReduceLROnPlateau (mode='min', factor=0.5, patience=5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(loader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Validating'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training for {NUM_EPOCHS} epochs\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate (ReduceLROnPlateau uses validation loss)\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model (based on validation accuracy)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'history': history\n",
        "        }, MODEL_SAVE_PATH)\n",
        "        print(f\"Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Curves Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curve\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curve\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('resnet_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Load Best Model and Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print overall accuracy score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - ResNet-18 Model', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('resnet_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and print the full classification report\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Test Accuracy**: Calculated above\n",
        "- **Best Validation Accuracy**: Calculated above\n",
        "- **Model saved to**: `resnet_model.pth`\n",
        "\n",
        "### Architecture Features:\n",
        "- Full ResNet-18 implementation from scratch\n",
        "- BasicBlock with skip connections (residual connections)\n",
        "- Batch Normalization after all convolutional layers\n",
        "- He/Kaiming initialization for optimal training from scratch\n",
        "- Adam Optimizer (lr=0.001) with ReduceLROnPlateau Learning Rate Scheduler\n",
        "\n",
        "### Files Generated:\n",
        "1. Trained model: `resnet_model.pth`\n",
        "2. Training curves: `resnet_training_curves.png`\n",
        "3. Confusion matrix: `resnet_confusion_matrix.png`\n",
        "\n",
        "All files are saved in the current directory.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
